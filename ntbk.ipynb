{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from utils import *\n",
    "from models import *\n",
    "from attention import *\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from sklearn.model_selection import train_test_split\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== THE DATASET IS READY ========\n"
     ]
    }
   ],
   "source": [
    "# Download the file\n",
    "get_file()\n",
    "path_to_file = 'spa-eng/spa.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of line in the dataset is 118964\n"
     ]
    }
   ],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 118960\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107064 107064 11896 11896\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.1)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "662 ----> miro\n",
      "24 ----> su\n",
      "397 ----> reloj\n",
      "33 ----> y\n",
      "16 ----> se\n",
      "213 ----> dio\n",
      "264 ----> cuenta\n",
      "5 ----> de\n",
      "4 ----> que\n",
      "605 ----> eran\n",
      "34 ----> mas\n",
      "5 ----> de\n",
      "32 ----> las\n",
      "312 ----> cinco\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "27 ----> she\n",
      "331 ----> looked\n",
      "44 ----> at\n",
      "50 ----> her\n",
      "292 ----> watch\n",
      "42 ----> and\n",
      "8532 ----> noted\n",
      "17 ----> that\n",
      "14 ----> it\n",
      "19 ----> was\n",
      "912 ----> past\n",
      "354 ----> five\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to torch tensor\n",
    "tensor_x = torch.Tensor(input_tensor_train).long() \n",
    "tensor_y = torch.Tensor(target_tensor_train).long()\n",
    "# create your datset\n",
    "my_dataset = data.TensorDataset(tensor_x,tensor_y) \n",
    "# create your dataloader\n",
    "my_dataloader = data.DataLoader(my_dataset,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=True,\n",
    "                        drop_last=True,\n",
    "                        num_workers=4)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_batch, example_target_batch = next(iter(my_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 42])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) torch.Size([64, 42, 1024])\n",
      "Encoder Hidden state shape: (batch size, units) torch.Size([1, 64, 1024])\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) torch.Size([64, 1024])\n",
      "Attention weights shape: (batch_size, sequence_length, 1) torch.Size([64, 42, 1])\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10, 1024)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) torch.Size([64, 12928])\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, 1024)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(torch.randint(1, 20, (BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE).to(device)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, 1024).to(device)\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters())\n",
    "decoder_optimizer = optim.Adam(decoder.parameters())\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 9.2146\n",
      "Epoch 1 Batch 100 Loss 0.9029\n",
      "Epoch 1 Batch 200 Loss 0.8691\n",
      "Epoch 1 Batch 300 Loss 0.7998\n",
      "Epoch 1 Batch 400 Loss 0.7791\n",
      "Epoch 1 Batch 500 Loss 0.6318\n",
      "Epoch 1 Batch 600 Loss 0.5529\n",
      "Epoch 1 Batch 700 Loss 0.5898\n",
      "Epoch 1 Batch 800 Loss 0.6158\n",
      "Epoch 1 Batch 900 Loss 0.5319\n",
      "Epoch 1 Batch 1000 Loss 0.4686\n",
      "Epoch 1 Batch 1100 Loss 0.4052\n",
      "Epoch 1 Batch 1200 Loss 0.4606\n",
      "Epoch 1 Batch 1300 Loss 0.4735\n",
      "Epoch 1 Batch 1400 Loss 0.4081\n",
      "Epoch 1 Batch 1500 Loss 0.3777\n",
      "Epoch 1 Batch 1600 Loss 0.3624\n",
      "Epoch 1 Loss 0.5909\n",
      "Time taken for 1 epoch 1108.2659974098206 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.3611\n",
      "Epoch 2 Batch 100 Loss 0.3215\n",
      "Epoch 2 Batch 200 Loss 0.2829\n",
      "Epoch 2 Batch 300 Loss 0.3506\n",
      "Epoch 2 Batch 400 Loss 0.2712\n",
      "Epoch 2 Batch 500 Loss 0.3108\n",
      "Epoch 2 Batch 600 Loss 0.2982\n",
      "Epoch 2 Batch 700 Loss 0.2652\n",
      "Epoch 2 Batch 800 Loss 0.3157\n",
      "Epoch 2 Batch 900 Loss 0.2315\n",
      "Epoch 2 Batch 1000 Loss 0.2689\n",
      "Epoch 2 Batch 1100 Loss 0.3285\n",
      "Epoch 2 Batch 1200 Loss 0.2742\n",
      "Epoch 2 Batch 1300 Loss 0.2649\n",
      "Epoch 2 Batch 1400 Loss 0.2457\n",
      "Epoch 2 Batch 1500 Loss 0.2931\n",
      "Epoch 2 Batch 1600 Loss 0.2444\n",
      "Epoch 2 Loss 0.2886\n",
      "Time taken for 1 epoch 1118.1558454036713 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.2285\n",
      "Epoch 3 Batch 100 Loss 0.1973\n",
      "Epoch 3 Batch 200 Loss 0.2143\n",
      "Epoch 3 Batch 300 Loss 0.1986\n",
      "Epoch 3 Batch 400 Loss 0.1905\n",
      "Epoch 3 Batch 500 Loss 0.1825\n",
      "Epoch 3 Batch 600 Loss 0.2008\n",
      "Epoch 3 Batch 700 Loss 0.2140\n",
      "Epoch 3 Batch 800 Loss 0.1572\n",
      "Epoch 3 Batch 900 Loss 0.1564\n",
      "Epoch 3 Batch 1000 Loss 0.1890\n",
      "Epoch 3 Batch 1100 Loss 0.1691\n",
      "Epoch 3 Batch 1200 Loss 0.1700\n",
      "Epoch 3 Batch 1300 Loss 0.1841\n",
      "Epoch 3 Batch 1400 Loss 0.1987\n",
      "Epoch 3 Batch 1500 Loss 0.1920\n",
      "Epoch 3 Batch 1600 Loss 0.1867\n",
      "Epoch 3 Loss 0.1907\n",
      "Time taken for 1 epoch 1097.1569232940674 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1070\n",
      "Epoch 4 Batch 100 Loss 0.0982\n",
      "Epoch 4 Batch 200 Loss 0.1651\n",
      "Epoch 4 Batch 300 Loss 0.1170\n",
      "Epoch 4 Batch 400 Loss 0.1159\n",
      "Epoch 4 Batch 500 Loss 0.1278\n",
      "Epoch 4 Batch 600 Loss 0.1462\n",
      "Epoch 4 Batch 700 Loss 0.1362\n",
      "Epoch 4 Batch 800 Loss 0.1165\n",
      "Epoch 4 Batch 900 Loss 0.1304\n",
      "Epoch 4 Batch 1000 Loss 0.1141\n",
      "Epoch 4 Batch 1100 Loss 0.1075\n",
      "Epoch 4 Batch 1200 Loss 0.1333\n",
      "Epoch 4 Batch 1300 Loss 0.1685\n",
      "Epoch 4 Batch 1400 Loss 0.1718\n",
      "Epoch 4 Batch 1500 Loss 0.1335\n",
      "Epoch 4 Batch 1600 Loss 0.1470\n",
      "Epoch 4 Loss 0.1353\n",
      "Time taken for 1 epoch 1088.8594372272491 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0803\n",
      "Epoch 5 Batch 100 Loss 0.0756\n",
      "Epoch 5 Batch 200 Loss 0.1175\n",
      "Epoch 5 Batch 300 Loss 0.0960\n",
      "Epoch 5 Batch 400 Loss 0.0813\n",
      "Epoch 5 Batch 500 Loss 0.0605\n",
      "Epoch 5 Batch 600 Loss 0.0670\n",
      "Epoch 5 Batch 700 Loss 0.1001\n",
      "Epoch 5 Batch 800 Loss 0.1123\n",
      "Epoch 5 Batch 900 Loss 0.1054\n",
      "Epoch 5 Batch 1000 Loss 0.1133\n",
      "Epoch 5 Batch 1100 Loss 0.0921\n",
      "Epoch 5 Batch 1200 Loss 0.1193\n",
      "Epoch 5 Batch 1300 Loss 0.1222\n",
      "Epoch 5 Batch 1400 Loss 0.1087\n",
      "Epoch 5 Batch 1500 Loss 0.1310\n",
      "Epoch 5 Batch 1600 Loss 0.0890\n",
      "Epoch 5 Loss 0.1006\n",
      "Time taken for 1 epoch 1089.2747201919556 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0697\n",
      "Epoch 6 Batch 100 Loss 0.0567\n",
      "Epoch 6 Batch 200 Loss 0.0691\n",
      "Epoch 6 Batch 300 Loss 0.0641\n",
      "Epoch 6 Batch 400 Loss 0.0905\n",
      "Epoch 6 Batch 500 Loss 0.0698\n",
      "Epoch 6 Batch 600 Loss 0.1152\n",
      "Epoch 6 Batch 700 Loss 0.0578\n",
      "Epoch 6 Batch 800 Loss 0.0907\n",
      "Epoch 6 Batch 900 Loss 0.0911\n",
      "Epoch 6 Batch 1000 Loss 0.0852\n",
      "Epoch 6 Batch 1100 Loss 0.0925\n",
      "Epoch 6 Batch 1200 Loss 0.0911\n",
      "Epoch 6 Batch 1300 Loss 0.0712\n",
      "Epoch 6 Batch 1400 Loss 0.0978\n",
      "Epoch 6 Batch 1500 Loss 0.0949\n",
      "Epoch 6 Batch 1600 Loss 0.0909\n",
      "Epoch 6 Loss 0.0772\n",
      "Time taken for 1 epoch 1088.7453951835632 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0632\n",
      "Epoch 7 Batch 100 Loss 0.0481\n",
      "Epoch 7 Batch 200 Loss 0.0618\n",
      "Epoch 7 Batch 300 Loss 0.0464\n",
      "Epoch 7 Batch 400 Loss 0.0543\n",
      "Epoch 7 Batch 500 Loss 0.0571\n",
      "Epoch 7 Batch 600 Loss 0.0417\n",
      "Epoch 7 Batch 700 Loss 0.0507\n",
      "Epoch 7 Batch 800 Loss 0.0730\n",
      "Epoch 7 Batch 900 Loss 0.0582\n",
      "Epoch 7 Batch 1000 Loss 0.0614\n",
      "Epoch 7 Batch 1100 Loss 0.0708\n",
      "Epoch 7 Batch 1200 Loss 0.0629\n",
      "Epoch 7 Batch 1300 Loss 0.0692\n",
      "Epoch 7 Batch 1400 Loss 0.0998\n",
      "Epoch 7 Batch 1500 Loss 0.0711\n",
      "Epoch 7 Batch 1600 Loss 0.0676\n",
      "Epoch 7 Loss 0.0668\n",
      "Time taken for 1 epoch 1094.0772926807404 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0457\n",
      "Epoch 8 Batch 100 Loss 0.0532\n",
      "Epoch 8 Batch 200 Loss 0.0342\n",
      "Epoch 8 Batch 300 Loss 0.0364\n",
      "Epoch 8 Batch 400 Loss 0.0441\n",
      "Epoch 8 Batch 500 Loss 0.0530\n",
      "Epoch 8 Batch 600 Loss 0.0376\n",
      "Epoch 8 Batch 700 Loss 0.0634\n",
      "Epoch 8 Batch 800 Loss 0.0418\n",
      "Epoch 8 Batch 900 Loss 0.0594\n",
      "Epoch 8 Batch 1000 Loss 0.0617\n",
      "Epoch 8 Batch 1100 Loss 0.0458\n",
      "Epoch 8 Batch 1200 Loss 0.0561\n",
      "Epoch 8 Batch 1300 Loss 0.0481\n",
      "Epoch 8 Batch 1400 Loss 0.0656\n",
      "Epoch 8 Batch 1500 Loss 0.0755\n",
      "Epoch 8 Batch 1600 Loss 0.0825\n",
      "Epoch 8 Loss 0.0527\n",
      "Time taken for 1 epoch 1139.7754883766174 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0426\n",
      "Epoch 9 Batch 100 Loss 0.0302\n",
      "Epoch 9 Batch 200 Loss 0.0482\n",
      "Epoch 9 Batch 300 Loss 0.0280\n",
      "Epoch 9 Batch 400 Loss 0.0326\n",
      "Epoch 9 Batch 500 Loss 0.0321\n",
      "Epoch 9 Batch 600 Loss 0.0502\n",
      "Epoch 9 Batch 700 Loss 0.0460\n",
      "Epoch 9 Batch 800 Loss 0.0528\n",
      "Epoch 9 Batch 900 Loss 0.0343\n",
      "Epoch 9 Batch 1000 Loss 0.0587\n",
      "Epoch 9 Batch 1100 Loss 0.0534\n",
      "Epoch 9 Batch 1200 Loss 0.0609\n",
      "Epoch 9 Batch 1300 Loss 0.0384\n",
      "Epoch 9 Batch 1400 Loss 0.0542\n",
      "Epoch 9 Batch 1500 Loss 0.0646\n",
      "Epoch 9 Batch 1600 Loss 0.0674\n",
      "Epoch 9 Loss 0.0469\n",
      "Time taken for 1 epoch 1116.2289907932281 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0272\n",
      "Epoch 10 Batch 100 Loss 0.0317\n",
      "Epoch 10 Batch 200 Loss 0.0497\n",
      "Epoch 10 Batch 300 Loss 0.0485\n",
      "Epoch 10 Batch 400 Loss 0.0428\n",
      "Epoch 10 Batch 500 Loss 0.0423\n",
      "Epoch 10 Batch 600 Loss 0.0719\n",
      "Epoch 10 Batch 700 Loss 0.0573\n",
      "Epoch 10 Batch 800 Loss 0.0709\n",
      "Epoch 10 Batch 900 Loss 0.0428\n",
      "Epoch 10 Batch 1000 Loss 0.0413\n",
      "Epoch 10 Batch 1100 Loss 0.0444\n",
      "Epoch 10 Batch 1200 Loss 0.0518\n",
      "Epoch 10 Batch 1300 Loss 0.0488\n",
      "Epoch 10 Batch 1400 Loss 0.0447\n",
      "Epoch 10 Batch 1500 Loss 0.0718\n",
      "Epoch 10 Batch 1600 Loss 0.0550\n",
      "Epoch 10 Loss 0.0461\n",
      "Time taken for 1 epoch 1131.700609445572 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0214\n",
      "Epoch 11 Batch 100 Loss 0.0490\n",
      "Epoch 11 Batch 200 Loss 0.0243\n",
      "Epoch 11 Batch 300 Loss 0.0238\n",
      "Epoch 11 Batch 400 Loss 0.0268\n",
      "Epoch 11 Batch 500 Loss 0.0270\n",
      "Epoch 11 Batch 600 Loss 0.0555\n",
      "Epoch 11 Batch 700 Loss 0.0353\n",
      "Epoch 11 Batch 800 Loss 0.0438\n",
      "Epoch 11 Batch 900 Loss 0.0368\n",
      "Epoch 11 Batch 1000 Loss 0.0511\n",
      "Epoch 11 Batch 1100 Loss 0.0510\n",
      "Epoch 11 Batch 1200 Loss 0.0410\n",
      "Epoch 11 Batch 1300 Loss 0.0781\n",
      "Epoch 11 Batch 1400 Loss 0.0568\n",
      "Epoch 11 Batch 1500 Loss 0.0664\n",
      "Epoch 11 Batch 1600 Loss 0.0707\n",
      "Epoch 11 Loss 0.0446\n",
      "Time taken for 1 epoch 1119.1716153621674 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0314\n",
      "Epoch 12 Batch 100 Loss 0.0333\n",
      "Epoch 12 Batch 200 Loss 0.0374\n",
      "Epoch 12 Batch 300 Loss 0.0368\n",
      "Epoch 12 Batch 400 Loss 0.0331\n",
      "Epoch 12 Batch 500 Loss 0.0336\n",
      "Epoch 12 Batch 600 Loss 0.0383\n",
      "Epoch 12 Batch 700 Loss 0.0369\n",
      "Epoch 12 Batch 800 Loss 0.0308\n",
      "Epoch 12 Batch 900 Loss 0.0539\n",
      "Epoch 12 Batch 1000 Loss 0.0285\n",
      "Epoch 12 Batch 1100 Loss 0.0301\n",
      "Epoch 12 Batch 1200 Loss 0.0417\n",
      "Epoch 12 Batch 1300 Loss 0.0360\n",
      "Epoch 12 Batch 1400 Loss 0.0438\n",
      "Epoch 12 Batch 1500 Loss 0.0472\n",
      "Epoch 12 Batch 1600 Loss 0.0634\n",
      "Epoch 12 Loss 0.0380\n",
      "Time taken for 1 epoch 1139.3502824306488 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.0292\n",
      "Epoch 13 Batch 100 Loss 0.0253\n",
      "Epoch 13 Batch 200 Loss 0.0304\n",
      "Epoch 13 Batch 300 Loss 0.0262\n",
      "Epoch 13 Batch 400 Loss 0.0392\n",
      "Epoch 13 Batch 500 Loss 0.0355\n",
      "Epoch 13 Batch 600 Loss 0.0397\n",
      "Epoch 13 Batch 700 Loss 0.0300\n",
      "Epoch 13 Batch 800 Loss 0.0340\n",
      "Epoch 13 Batch 900 Loss 0.0351\n",
      "Epoch 13 Batch 1000 Loss 0.0465\n",
      "Epoch 13 Batch 1100 Loss 0.0471\n",
      "Epoch 13 Batch 1200 Loss 0.0405\n",
      "Epoch 13 Batch 1300 Loss 0.0494\n",
      "Epoch 13 Batch 1400 Loss 0.0472\n",
      "Epoch 13 Batch 1500 Loss 0.0550\n",
      "Epoch 13 Batch 1600 Loss 0.0407\n",
      "Epoch 13 Loss 0.0364\n",
      "Time taken for 1 epoch 1132.9205193519592 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0390\n",
      "Epoch 14 Batch 100 Loss 0.0164\n",
      "Epoch 14 Batch 200 Loss 0.0254\n",
      "Epoch 14 Batch 300 Loss 0.0264\n",
      "Epoch 14 Batch 400 Loss 0.0309\n",
      "Epoch 14 Batch 500 Loss 0.0225\n",
      "Epoch 14 Batch 600 Loss 0.0329\n",
      "Epoch 14 Batch 700 Loss 0.0418\n",
      "Epoch 14 Batch 800 Loss 0.0420\n",
      "Epoch 14 Batch 900 Loss 0.0378\n",
      "Epoch 14 Batch 1000 Loss 0.0309\n",
      "Epoch 14 Batch 1100 Loss 0.0391\n",
      "Epoch 14 Batch 1200 Loss 0.0516\n",
      "Epoch 14 Batch 1300 Loss 0.0327\n",
      "Epoch 14 Batch 1400 Loss 0.0391\n",
      "Epoch 14 Batch 1500 Loss 0.0410\n",
      "Epoch 14 Batch 1600 Loss 0.0384\n",
      "Epoch 14 Loss 0.0369\n",
      "Time taken for 1 epoch 1104.9438977241516 sec\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Batch 0 Loss 0.0246\n",
      "Epoch 15 Batch 100 Loss 0.0227\n",
      "Epoch 15 Batch 200 Loss 0.0158\n",
      "Epoch 15 Batch 300 Loss 0.0335\n",
      "Epoch 15 Batch 400 Loss 0.0354\n",
      "Epoch 15 Batch 500 Loss 0.0239\n",
      "Epoch 15 Batch 600 Loss 0.0403\n",
      "Epoch 15 Batch 700 Loss 0.0277\n",
      "Epoch 15 Batch 800 Loss 0.0329\n",
      "Epoch 15 Batch 900 Loss 0.0351\n",
      "Epoch 15 Batch 1000 Loss 0.0297\n",
      "Epoch 15 Batch 1100 Loss 0.0263\n",
      "Epoch 15 Batch 1200 Loss 0.0450\n",
      "Epoch 15 Batch 1300 Loss 0.0341\n",
      "Epoch 15 Batch 1400 Loss 0.0413\n",
      "Epoch 15 Batch 1500 Loss 0.0342\n",
      "Epoch 15 Batch 1600 Loss 0.0382\n",
      "Epoch 15 Loss 0.0333\n",
      "Time taken for 1 epoch 1092.897058725357 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.0235\n",
      "Epoch 16 Batch 100 Loss 0.0250\n",
      "Epoch 16 Batch 200 Loss 0.0279\n",
      "Epoch 16 Batch 300 Loss 0.0226\n",
      "Epoch 16 Batch 400 Loss 0.0293\n",
      "Epoch 16 Batch 500 Loss 0.0270\n",
      "Epoch 16 Batch 600 Loss 0.0300\n",
      "Epoch 16 Batch 700 Loss 0.0243\n",
      "Epoch 16 Batch 800 Loss 0.0367\n",
      "Epoch 16 Batch 900 Loss 0.0244\n",
      "Epoch 16 Batch 1000 Loss 0.0430\n",
      "Epoch 16 Batch 1100 Loss 0.0536\n",
      "Epoch 16 Batch 1200 Loss 0.0437\n",
      "Epoch 16 Batch 1300 Loss 0.0472\n",
      "Epoch 16 Batch 1400 Loss 0.0375\n",
      "Epoch 16 Batch 1500 Loss 0.0503\n",
      "Epoch 16 Batch 1600 Loss 0.0495\n",
      "Epoch 16 Loss 0.0341\n",
      "Time taken for 1 epoch 1092.219352722168 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0237\n",
      "Epoch 17 Batch 100 Loss 0.0229\n",
      "Epoch 17 Batch 200 Loss 0.0272\n",
      "Epoch 17 Batch 300 Loss 0.0245\n",
      "Epoch 17 Batch 400 Loss 0.0161\n",
      "Epoch 17 Batch 500 Loss 0.0392\n",
      "Epoch 17 Batch 600 Loss 0.0310\n",
      "Epoch 17 Batch 700 Loss 0.0396\n",
      "Epoch 17 Batch 800 Loss 0.0339\n",
      "Epoch 17 Batch 900 Loss 0.0415\n",
      "Epoch 17 Batch 1000 Loss 0.0220\n",
      "Epoch 17 Batch 1100 Loss 0.0336\n",
      "Epoch 17 Batch 1200 Loss 0.0332\n",
      "Epoch 17 Batch 1300 Loss 0.0450\n",
      "Epoch 17 Batch 1400 Loss 0.0411\n",
      "Epoch 17 Batch 1500 Loss 0.0479\n",
      "Epoch 17 Batch 1600 Loss 0.0635\n",
      "Epoch 17 Loss 0.0337\n",
      "Time taken for 1 epoch 1091.9463300704956 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0314\n",
      "Epoch 18 Batch 100 Loss 0.0220\n",
      "Epoch 18 Batch 200 Loss 0.0335\n",
      "Epoch 18 Batch 300 Loss 0.0318\n",
      "Epoch 18 Batch 400 Loss 0.0275\n",
      "Epoch 18 Batch 500 Loss 0.0264\n",
      "Epoch 18 Batch 600 Loss 0.0351\n",
      "Epoch 18 Batch 700 Loss 0.0292\n",
      "Epoch 18 Batch 800 Loss 0.0336\n",
      "Epoch 18 Batch 900 Loss 0.0404\n",
      "Epoch 18 Batch 1000 Loss 0.0298\n",
      "Epoch 18 Batch 1100 Loss 0.0300\n",
      "Epoch 18 Batch 1200 Loss 0.0318\n",
      "Epoch 18 Batch 1300 Loss 0.0290\n",
      "Epoch 18 Batch 1400 Loss 0.0452\n",
      "Epoch 18 Batch 1500 Loss 0.0326\n",
      "Epoch 18 Batch 1600 Loss 0.0477\n",
      "Epoch 18 Loss 0.0335\n",
      "Time taken for 1 epoch 1113.886426448822 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0126\n",
      "Epoch 19 Batch 100 Loss 0.0274\n",
      "Epoch 19 Batch 200 Loss 0.0199\n",
      "Epoch 19 Batch 300 Loss 0.0255\n",
      "Epoch 19 Batch 400 Loss 0.0166\n",
      "Epoch 19 Batch 500 Loss 0.0254\n",
      "Epoch 19 Batch 600 Loss 0.0272\n",
      "Epoch 19 Batch 700 Loss 0.0392\n",
      "Epoch 19 Batch 800 Loss 0.0301\n",
      "Epoch 19 Batch 900 Loss 0.0275\n",
      "Epoch 19 Batch 1000 Loss 0.0308\n",
      "Epoch 19 Batch 1100 Loss 0.0440\n",
      "Epoch 19 Batch 1200 Loss 0.0474\n",
      "Epoch 19 Batch 1300 Loss 0.0530\n",
      "Epoch 19 Batch 1400 Loss 0.0315\n",
      "Epoch 19 Batch 1500 Loss 0.0498\n",
      "Epoch 19 Batch 1600 Loss 0.0412\n",
      "Epoch 19 Loss 0.0303\n",
      "Time taken for 1 epoch 1096.0280380249023 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0296\n",
      "Epoch 20 Batch 100 Loss 0.0296\n",
      "Epoch 20 Batch 200 Loss 0.0214\n",
      "Epoch 20 Batch 300 Loss 0.0288\n",
      "Epoch 20 Batch 400 Loss 0.0244\n",
      "Epoch 20 Batch 500 Loss 0.0196\n",
      "Epoch 20 Batch 600 Loss 0.0195\n",
      "Epoch 20 Batch 700 Loss 0.0292\n",
      "Epoch 20 Batch 800 Loss 0.0256\n",
      "Epoch 20 Batch 900 Loss 0.0335\n",
      "Epoch 20 Batch 1000 Loss 0.0312\n",
      "Epoch 20 Batch 1100 Loss 0.0320\n",
      "Epoch 20 Batch 1200 Loss 0.0300\n",
      "Epoch 20 Batch 1300 Loss 0.0375\n",
      "Epoch 20 Batch 1400 Loss 0.0281\n",
      "Epoch 20 Batch 1500 Loss 0.0383\n",
      "Epoch 20 Batch 1600 Loss 0.0333\n",
      "Epoch 20 Loss 0.0315\n",
      "Time taken for 1 epoch 1089.4565336704254 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(my_dataloader):\n",
    "        inp, targ = inp.to(device), targ.to(device)\n",
    "        batch_loss = train_step(inp, targ, encoder, decoder,\n",
    "                                encoder_optimizer, decoder_optimizer,\n",
    "                                criterion, device, BATCH_SIZE, targ_lang)\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss))\n",
    "            \n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        \n",
    "        pass\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> trata de averiguarlo . <end>\n",
      "Predicted translation: try to figure it out . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'trata de averiguarlo .', max_length_targ, max_length_inp, encoder, decoder, inp_lang, targ_lang, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> ¿ todavia estan en casa ? <end>\n",
      "Predicted translation: are you still home ? <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'¿ todavia estan en casa ?', max_length_targ, max_length_inp, encoder, decoder, inp_lang, targ_lang, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> esta es mi vida . <end>\n",
      "Predicted translation: this is my life . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'esta es mi vida .', max_length_targ, max_length_inp, encoder, decoder, inp_lang, targ_lang, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hace mucho frio aqui . <end>\n",
      "Predicted translation: it s very cold here . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'hace mucho frio aqui .', max_length_targ, max_length_inp, encoder, decoder, inp_lang, targ_lang, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
